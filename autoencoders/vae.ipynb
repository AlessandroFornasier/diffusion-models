{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlessandroFornasier/diffusion-models/blob/main/autoencoders/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kwHzd90uOsj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Autoencoder class.\n",
        "\n",
        "Args:\n",
        "  dims (List[int]): Dimensionality of the layers.\n",
        "  binary (bool): Flag to indicate binary data [0, 1]\n",
        "\"\"\"\n",
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self, dims : List[int], binary : bool = True):\n",
        "    super(Autoencoder, self).__init__()\n",
        "\n",
        "    self.binary = binary\n",
        "\n",
        "    \"\"\"\n",
        "    Encoder\n",
        "\n",
        "    Activation function:\n",
        "      SiLU\n",
        "\n",
        "    Note:\n",
        "      Latent space has no activation function\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for idim, odim in zip(dims[:-2], dims[1:-1]):\n",
        "      layers.append(nn.Linear(idim, odim))\n",
        "      layers.append(nn.SiLU())\n",
        "    layers.append(nn.Linear(dims[-2], dims[-1]))\n",
        "    self.encoder = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Decoder\n",
        "\n",
        "    Activation function for hidden layers:\n",
        "      SiLU\n",
        "\n",
        "    Activation function for output layer:\n",
        "      Sigmoid (normalizes (0, 1))\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for idim, odim in zip(dims[-2:0:-1], dims[-3::-1]):\n",
        "      layers.append(nn.Linear(idim, odim))\n",
        "      layers.append(nn.SiLU())\n",
        "    layers.append(nn.Linear(dims[1], dims[0]))\n",
        "    if self.binary:\n",
        "      layers.append(nn.Sigmoid())\n",
        "    self.decoder = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def encode(self, x)\n",
        "      \"\"\"\n",
        "      Encodes the input data into the latent space.\n",
        "\n",
        "      Args:\n",
        "        x (torch.Tensor): Input data.\n",
        "\n",
        "      Returns:\n",
        "        torch.Tensor: Encoded data, latent space.\n",
        "      \"\"\"\n",
        "      return self.encoder(x)\n",
        "\n",
        "    def decode(self, z)\n",
        "      \"\"\"\n",
        "      Decodes the latent space data.\n",
        "\n",
        "      Args:\n",
        "        z (torch.Tensor): Latent space data.\n",
        "\n",
        "      Returns:\n",
        "        torch.Tensor: Decoded data, output space.\n",
        "      \"\"\"\n",
        "      return self.decoder(z)\n",
        "\n",
        "    \"\"\"\n",
        "    Forward pass of the autoencoder\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "      return self.decode(self.encode(x))"
      ],
      "metadata": {
        "id": "Y3ORNf2C0VFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Autoencoder class.\n",
        "\n",
        "Args:\n",
        "  dims (List[int]): Dimensionality of the layers.\n",
        "  binary (bool): Flag to indicate binary data [0, 1]\n",
        "\"\"\"\n",
        "class VAE(Autoencoder):\n",
        "  def __init__(self, dims):\n",
        "    super().__init__(dims)\n",
        "\n",
        "  def encode(self, x, eps: float = 1e-9):\n",
        "    pass\n",
        "\n",
        "  def reparametrize(self, dist):\n",
        "    pass\n",
        "\n",
        "  def decode(self, z):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, compute_loss: bool = True):\n",
        "    pass"
      ],
      "metadata": {
        "id": "cdvtWx-h7UjI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}